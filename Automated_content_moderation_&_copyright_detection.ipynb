{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w5M7-YurGGLA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "class TextModerationSystem:\n",
        "    def __init__(self):\n",
        "        # Initialize pre-trained models\n",
        "        self.toxicity_classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"unitary/toxic-bert\"\n",
        "        )\n",
        "        self.spam_detector = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"madhurjindal/autonlp-Gibberish-Detector-492513457\"\n",
        "        )\n",
        "\n",
        "        # Define moderation categories for your platform\n",
        "        self.moderation_rules = {\n",
        "            'spam_keywords': [\n",
        "                'buy followers', 'fake engagement', 'guaranteed viral',\n",
        "                'instant fame', 'bot followers', 'cheap views'\n",
        "            ],\n",
        "            'inappropriate_contact': [\n",
        "                'whatsapp', 'telegram', 'personal phone', 'meet offline'\n",
        "            ],\n",
        "            'scam_patterns': [\n",
        "                'send money first', 'payment upfront', 'wire transfer',\n",
        "                'bitcoin payment', 'western union'\n",
        "            ]\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate_message(self, text, context=\"general\"):\n",
        "    \"\"\"\n",
        "    Moderate text messages between influencers and brands\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"approved\": True,\n",
        "        \"confidence\": 0.0,\n",
        "        \"flags\": [],\n",
        "        \"severity\": \"safe\"\n",
        "    }\n",
        "\n",
        "    # 1. Toxicity Detection\n",
        "    toxicity_result = self.toxicity_classifier(text)\n",
        "    toxicity_score = toxicity_result[0]['score'] if toxicity_result[0]['label'] == 'TOXIC' else 0\n",
        "\n",
        "    if toxicity_score > 0.8:\n",
        "        results[\"approved\"] = False\n",
        "        results[\"flags\"].append(\"high_toxicity\")\n",
        "        results[\"severity\"] = \"high\"\n",
        "    elif toxicity_score > 0.5:\n",
        "        results[\"flags\"].append(\"moderate_toxicity\")\n",
        "        results[\"severity\"] = \"medium\"\n",
        "\n",
        "    # 2. Spam Detection\n",
        "    spam_result = self.spam_detector(text)\n",
        "    if spam_result[0]['label'] == 'spam' and spam_result[0]['score'] > 0.7:\n",
        "        results[\"approved\"] = False\n",
        "        results[\"flags\"].append(\"spam_content\")\n",
        "\n",
        "    # 3. Platform-specific rule checking\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check for spam keywords\n",
        "    for keyword in self.moderation_rules['spam_keywords']:\n",
        "        if keyword in text_lower:\n",
        "            results[\"flags\"].append(f\"spam_keyword: {keyword}\")\n",
        "            results[\"severity\"] = \"medium\"\n",
        "\n",
        "    # Check for inappropriate contact attempts\n",
        "    for contact_attempt in self.moderation_rules['inappropriate_contact']:\n",
        "        if contact_attempt in text_lower:\n",
        "            results[\"flags\"].append(\"inappropriate_contact_sharing\")\n",
        "            results[\"severity\"] = \"medium\"\n",
        "\n",
        "    # Check for scam patterns\n",
        "    for scam_pattern in self.moderation_rules['scam_patterns']:\n",
        "        if scam_pattern in text_lower:\n",
        "            results[\"approved\"] = False\n",
        "            results[\"flags\"].append(\"potential_scam\")\n",
        "            results[\"severity\"] = \"high\"\n",
        "\n",
        "    results[\"confidence\"] = max(toxicity_score, 0.5 if results[\"flags\"] else 0.1)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "IhrroITgSg-n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def moderate_profile_data(self, profile_data):\n",
        "    \"\"\"\n",
        "    Moderate influencer/brand profile information\n",
        "    \"\"\"\n",
        "    moderation_results = {}\n",
        "\n",
        "    # Moderate different profile fields\n",
        "    fields_to_check = ['bio', 'description', 'company_name', 'tagline']\n",
        "\n",
        "    for field in fields_to_check:\n",
        "        if field in profile_data:\n",
        "            field_result = self.moderate_message(\n",
        "                profile_data[field],\n",
        "                context=\"profile\"\n",
        "            )\n",
        "            moderation_results[field] = field_result\n",
        "\n",
        "    # Overall profile approval\n",
        "    overall_approved = all(\n",
        "        result[\"approved\"] for result in moderation_results.values()\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"profile_approved\": overall_approved,\n",
        "        \"field_results\": moderation_results,\n",
        "        \"action_required\": not overall_approved\n",
        "    }\n"
      ],
      "metadata": {
        "id": "jfIt6loUSzpc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIXED MODERATION SYSTEM (Case Sensitivity Bug Fixed) ---\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "class FixedModerationSystem:\n",
        "    def __init__(self):\n",
        "        print(\"ğŸ”§ Loading AI moderation model...\")\n",
        "\n",
        "        self.toxicity_classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"unitary/toxic-bert\"\n",
        "        )\n",
        "\n",
        "        # Platform-specific rules\n",
        "        self.platform_rules = {\n",
        "            'spam_keywords': [\n",
        "                'buy followers', 'fake followers', 'guaranteed viral',\n",
        "                'instant fame', 'cheap views', 'bot followers'\n",
        "            ],\n",
        "            'scam_patterns': [\n",
        "                'send money first', 'payment upfront', 'wire transfer',\n",
        "                'bitcoin payment', 'advance payment'\n",
        "            ],\n",
        "            'contact_sharing': [\n",
        "                'whatsapp', 'telegram', 'my number', 'call me at'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        print(\"âœ… Fixed moderation system ready!\")\n",
        "\n",
        "    def moderate_message(self, text):\n",
        "        \"\"\"\n",
        "        FIXED: Case-insensitive AI toxicity detection\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"approved\": True,\n",
        "            \"confidence\": 0.0,\n",
        "            \"flags\": [],\n",
        "            \"ai_decision\": None\n",
        "        }\n",
        "\n",
        "        # 1. AI Toxicity Detection (FIXED: case-insensitive)\n",
        "        try:\n",
        "            ai_result = self.toxicity_classifier(text)\n",
        "\n",
        "            for item in ai_result:\n",
        "                label = item['label'].lower()  # FIXED: Convert to lowercase\n",
        "                score = item['score']\n",
        "\n",
        "                results[\"ai_decision\"] = {\n",
        "                    \"label\": label,\n",
        "                    \"score\": score,\n",
        "                    \"original_label\": item['label']  # Keep original for debugging\n",
        "                }\n",
        "\n",
        "                # FIXED: Check for lowercase 'toxic'\n",
        "                if label == 'toxic' and score > 0.5:\n",
        "                    results[\"approved\"] = False\n",
        "                    results[\"flags\"].append(\"ai_toxic\")\n",
        "                    results[\"confidence\"] = score\n",
        "                    print(f\"ğŸš« BLOCKING: AI detected toxic content (score: {score:.3f})\")\n",
        "                    return results\n",
        "                elif label == 'toxic':\n",
        "                    results[\"confidence\"] = score\n",
        "                    print(f\"âš ï¸ Low toxicity detected (score: {score:.3f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ AI Error: {e}\")\n",
        "\n",
        "        # 2. Platform-specific rules\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Spam detection\n",
        "        for spam in self.platform_rules['spam_keywords']:\n",
        "            if spam in text_lower:\n",
        "                results[\"approved\"] = False\n",
        "                results[\"flags\"].append(\"spam\")\n",
        "                results[\"confidence\"] = 0.8\n",
        "                return results\n",
        "\n",
        "        # Scam detection\n",
        "        for scam in self.platform_rules['scam_patterns']:\n",
        "            if scam in text_lower:\n",
        "                results[\"approved\"] = False\n",
        "                results[\"flags\"].append(\"scam\")\n",
        "                results[\"confidence\"] = 0.9\n",
        "                return results\n",
        "\n",
        "        # Contact sharing (flag only)\n",
        "        for contact in self.platform_rules['contact_sharing']:\n",
        "            if contact in text_lower:\n",
        "                results[\"flags\"].append(\"contact_sharing\")\n",
        "\n",
        "        # Set default confidence if not set by AI\n",
        "        if results[\"confidence\"] == 0.0:\n",
        "            results[\"confidence\"] = 0.1\n",
        "\n",
        "        return results\n",
        "\n",
        "def fixed_display(text, result):\n",
        "    \"\"\"Clean result display\"\"\"\n",
        "    print(f\"\\nğŸ“ TESTING: '{text}'\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Status with clear indication\n",
        "    if result[\"approved\"]:\n",
        "        print(\"âœ… STATUS: APPROVED\")\n",
        "    else:\n",
        "        print(\"âŒ STATUS: BLOCKED\")\n",
        "\n",
        "    print(f\"ğŸ¯ CONFIDENCE: {result['confidence']:.3f}\")\n",
        "\n",
        "    if result[\"flags\"]:\n",
        "        print(f\"ğŸš© FLAGS: {', '.join(result['flags'])}\")\n",
        "\n",
        "    # Show AI decision details\n",
        "    if result.get(\"ai_decision\"):\n",
        "        ai_info = result[\"ai_decision\"]\n",
        "        print(f\"ğŸ¤– AI DECISION:\")\n",
        "        print(f\"   Label: {ai_info['label']} (original: {ai_info['original_label']})\")\n",
        "        print(f\"   Score: {ai_info['score']:.3f}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def test_fixed_system():\n",
        "    \"\"\"Test the fixed moderation system\"\"\"\n",
        "    moderator = FixedModerationSystem()\n",
        "\n",
        "    # Test the cases that were failing\n",
        "    print(\"\\nğŸ§ª TESTING PREVIOUSLY FAILING CASES:\")\n",
        "    failing_cases = [\"fuck you\", \"kill you\", \"hello there\", \"buy followers\"]\n",
        "\n",
        "    for test_text in failing_cases:\n",
        "        result = moderator.moderate_message(test_text)\n",
        "        fixed_display(test_text, result)\n",
        "\n",
        "    print(\"\\nğŸ”¬ INTERACTIVE TESTING:\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter message to test (or 'quit'): \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        elif not user_input:\n",
        "            continue\n",
        "\n",
        "        result = moderator.moderate_message(user_input)\n",
        "        fixed_display(user_input, result)\n",
        "\n",
        "# Run the fixed system\n",
        "test_fixed_system()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoBcgVAYS3LE",
        "outputId": "11be998a-cba5-4f89-f53a-135ffcf34ae1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Loading AI moderation model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fixed moderation system ready!\n",
            "\n",
            "ğŸ§ª TESTING PREVIOUSLY FAILING CASES:\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.998)\n",
            "\n",
            "ğŸ“ TESTING: 'fuck you'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.998\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.998\n",
            "==================================================\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.922)\n",
            "\n",
            "ğŸ“ TESTING: 'kill you'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.922\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.922\n",
            "==================================================\n",
            "âš ï¸ Low toxicity detected (score: 0.001)\n",
            "\n",
            "ğŸ“ TESTING: 'hello there'\n",
            "==================================================\n",
            "âœ… STATUS: APPROVED\n",
            "ğŸ¯ CONFIDENCE: 0.001\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.001\n",
            "==================================================\n",
            "âš ï¸ Low toxicity detected (score: 0.003)\n",
            "\n",
            "ğŸ“ TESTING: 'buy followers'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.800\n",
            "ğŸš© FLAGS: spam\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.003\n",
            "==================================================\n",
            "\n",
            "ğŸ”¬ INTERACTIVE TESTING:\n",
            "\n",
            "Enter message to test (or 'quit'): kill you\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.922)\n",
            "\n",
            "ğŸ“ TESTING: 'kill you'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.922\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.922\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): harras you\n",
            "âš ï¸ Low toxicity detected (score: 0.176)\n",
            "\n",
            "ğŸ“ TESTING: 'harras you'\n",
            "==================================================\n",
            "âœ… STATUS: APPROVED\n",
            "ğŸ¯ CONFIDENCE: 0.176\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.176\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): pay the money upfront\n",
            "âš ï¸ Low toxicity detected (score: 0.015)\n",
            "\n",
            "ğŸ“ TESTING: 'pay the money upfront'\n",
            "==================================================\n",
            "âœ… STATUS: APPROVED\n",
            "ğŸ¯ CONFIDENCE: 0.015\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.015\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): This shit doesn't work\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.975)\n",
            "\n",
            "ğŸ“ TESTING: 'This shit doesn't work'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.975\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.975\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): What the hell is wrong with you?\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.945)\n",
            "\n",
            "ğŸ“ TESTING: 'What the hell is wrong with you?'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.945\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.945\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): I'll hurt you badly\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.793)\n",
            "\n",
            "ğŸ“ TESTING: 'I'll hurt you badly'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.793\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.793\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): You're completely worthless\n",
            "ğŸš« BLOCKING: AI detected toxic content (score: 0.956)\n",
            "\n",
            "ğŸ“ TESTING: 'You're completely worthless'\n",
            "==================================================\n",
            "âŒ STATUS: BLOCKED\n",
            "ğŸ¯ CONFIDENCE: 0.956\n",
            "ğŸš© FLAGS: ai_toxic\n",
            "ğŸ¤– AI DECISION:\n",
            "   Label: toxic (original: toxic)\n",
            "   Score: 0.956\n",
            "==================================================\n",
            "\n",
            "Enter message to test (or 'quit'): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the toxicity classifier model, you can use the save_pretrained method.\n",
        "# First, instantiate the FixedModerationSystem to load the model\n",
        "moderation_system = FixedModerationSystem()\n",
        "\n",
        "# Now save the toxicity_classifier\n",
        "save_directory = \"./saved_toxicity_model\"\n",
        "moderation_system.toxicity_classifier.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Toxicity classifier model saved to {save_directory}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6jwHc9frAy5",
        "outputId": "fba5e131-2f40-4de7-b021-19245e6a8cee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Loading AI moderation model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fixed moderation system ready!\n",
            "Toxicity classifier model saved to ./saved_toxicity_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84450101",
        "outputId": "a57b3a5f-fbb5-4e54-db88-6ef27512a3dc"
      },
      "source": [
        "def interpret_toxicity_result(text, ai_result):\n",
        "    \"\"\"\n",
        "    Convert AI toxicity scores into actionable decisions\n",
        "    \"\"\"\n",
        "    score = ai_result[0]['score']\n",
        "\n",
        "    if score > 0.8:\n",
        "        return {\n",
        "            \"action\": \"block\",\n",
        "            \"reason\": \"high_toxicity\",\n",
        "            \"confidence\": score,\n",
        "            \"message\": f\"Content blocked - {score:.1%} toxic confidence\"\n",
        "        }\n",
        "    elif score > 0.5:\n",
        "        return {\n",
        "            \"action\": \"flag\",\n",
        "            \"reason\": \"moderate_toxicity\",\n",
        "            \"confidence\": score,\n",
        "            \"message\": f\"Content flagged for review - {score:.1%} toxic confidence\"\n",
        "        }\n",
        "    elif score > 0.3:\n",
        "        return {\n",
        "            \"action\": \"warn\",\n",
        "            \"reason\": \"borderline_content\",\n",
        "            \"confidence\": score,\n",
        "            \"message\": f\"Borderline content detected - {score:.1%} toxic confidence\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"action\": \"approve\",\n",
        "            \"reason\": \"safe_content\",\n",
        "            \"confidence\": score,\n",
        "            \"message\": f\"Content approved - {score:.1%} toxic confidence\"\n",
        "        }\n",
        "\n",
        "# Test with your examples\n",
        "test_texts = [\n",
        "    \"This is a completely harmless message.\",\n",
        "    \"You are a terrible person!\",\n",
        "    \"I hate everything about this.\",\n",
        "    \"What a wonderful day!\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ§ª INTERPRETING YOUR MODEL RESULTS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for text in test_texts:\n",
        "    ai_result = loaded_toxicity_classifier(text)\n",
        "    interpretation = interpret_toxicity_result(text, ai_result)\n",
        "\n",
        "    print(f\"\\nğŸ“ Text: '{text}'\")\n",
        "    print(f\"ğŸ¤– AI Score: {ai_result[0]['score']:.1%}\")\n",
        "    print(f\"âš¡ Action: {interpretation['action'].upper()}\")\n",
        "    print(f\"ğŸ’¬ Message: {interpretation['message']}\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª INTERPRETING YOUR MODEL RESULTS:\n",
            "==================================================\n",
            "\n",
            "ğŸ“ Text: 'This is a completely harmless message.'\n",
            "ğŸ¤– AI Score: 0.1%\n",
            "âš¡ Action: APPROVE\n",
            "ğŸ’¬ Message: Content approved - 0.1% toxic confidence\n",
            "\n",
            "ğŸ“ Text: 'You are a terrible person!'\n",
            "ğŸ¤– AI Score: 92.5%\n",
            "âš¡ Action: BLOCK\n",
            "ğŸ’¬ Message: Content blocked - 92.5% toxic confidence\n",
            "\n",
            "ğŸ“ Text: 'I hate everything about this.'\n",
            "ğŸ¤– AI Score: 31.9%\n",
            "âš¡ Action: WARN\n",
            "ğŸ’¬ Message: Borderline content detected - 31.9% toxic confidence\n",
            "\n",
            "ğŸ“ Text: 'What a wonderful day!'\n",
            "ğŸ¤– AI Score: 0.1%\n",
            "âš¡ Action: APPROVE\n",
            "ğŸ’¬ Message: Content approved - 0.1% toxic confidence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SAVE COMPLETE TOXICITY MODEL PIPELINE ---\n",
        "\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Your current loaded model\n",
        "toxicity_classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"unitary/toxic-bert\"\n",
        ")\n",
        "\n",
        "def save_toxicity_model_complete(model_pipeline, save_path=\"toxicity_model_v1\"):\n",
        "    \"\"\"\n",
        "    Save the complete toxicity model pipeline with metadata\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ’¾ Saving toxicity model to {save_path}...\")\n",
        "\n",
        "    # Create directory structure\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Save the model pipeline\n",
        "    model_pipeline.save_pretrained(save_path)\n",
        "\n",
        "    # Save model metadata\n",
        "    metadata = {\n",
        "        \"model_name\": \"unitary/toxic-bert\",\n",
        "        \"model_type\": \"toxicity_classifier\",\n",
        "        \"saved_date\": datetime.now().isoformat(),\n",
        "        \"performance_thresholds\": {\n",
        "            \"block_threshold\": 0.7,\n",
        "            \"flag_threshold\": 0.4,\n",
        "            \"approve_threshold\": 0.1\n",
        "        },\n",
        "        \"test_cases\": [\n",
        "            {\"text\": \"This is harmless\", \"expected_score\": \"< 0.1\"},\n",
        "            {\"text\": \"You are terrible\", \"expected_score\": \"> 0.8\"},\n",
        "            {\"text\": \"I hate this\", \"expected_score\": \"0.3-0.4\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_path, \"model_metadata.json\"), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… Model saved successfully to {save_path}/\")\n",
        "    return save_path\n",
        "\n",
        "# Save your model\n",
        "saved_path = save_toxicity_model_complete(toxicity_classifier)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz71RHkgsEdN",
        "outputId": "e8feadc4-568c-4f07-b12c-fa36ab7f94dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving toxicity model to toxicity_model_v1...\n",
            "âœ… Model saved successfully to toxicity_model_v1/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST SAVED TOXICITY MODEL ---\n",
        "\n",
        "from transformers import pipeline\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "def test_saved_toxicity_model():\n",
        "    \"\"\"Test your saved toxicity detection model\"\"\"\n",
        "    print(\"ğŸ§ª TESTING SAVED TOXICITY MODEL\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    try:\n",
        "        # Load the saved model (adjust path as needed)\n",
        "        loaded_model = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"./saved_toxicity_basic\"  # or your saved path\n",
        "        )\n",
        "        print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "        # Test cases to verify functionality\n",
        "        test_cases = [\n",
        "            (\"Safe content\", \"Hi! I'd love to collaborate on your campaign\"),\n",
        "            (\"Toxic content\", \"You are a terrible person\"),\n",
        "            (\"Spam content\", \"Buy 10k followers for $20\"),\n",
        "            (\"Borderline\", \"I really hate this situation\"),\n",
        "            (\"Professional\", \"When can we discuss partnership terms?\")\n",
        "        ]\n",
        "\n",
        "        print(\"\\nğŸ“‹ TESTING RESULTS:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for category, text in test_cases:\n",
        "            result = loaded_model(text)\n",
        "            score = result[0]['score']\n",
        "\n",
        "            # Determine action based on score\n",
        "            if score > 0.7:\n",
        "                action = \"ğŸš« BLOCK\"\n",
        "            elif score > 0.4:\n",
        "                action = \"âš ï¸ FLAG\"\n",
        "            else:\n",
        "                action = \"âœ… APPROVE\"\n",
        "\n",
        "            print(f\"{category.upper():12} | {action} | Score: {score:.3f}\")\n",
        "            print(f\"             | Text: '{text[:50]}...'\")\n",
        "            print()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading model: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "model_works = test_saved_toxicity_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSoFSqqGuPFH",
        "outputId": "f01c6e81-c5fa-4919-d06d-e405ea2ae214"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª TESTING SAVED TOXICITY MODEL\n",
            "========================================\n",
            "âŒ Error loading model: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './saved_toxicity_basic'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CHECK WHAT MODELS EXIST ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def check_saved_models():\n",
        "    \"\"\"Check what model files actually exist\"\"\"\n",
        "    print(\"ğŸ” CHECKING FOR SAVED MODELS\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Check current directory for model files\n",
        "    current_dir_files = os.listdir('.')\n",
        "    print(\"ğŸ“ Files in current directory:\")\n",
        "    for file in current_dir_files:\n",
        "        if 'model' in file.lower() or 'toxicity' in file.lower():\n",
        "            print(f\"  â€¢ {file}\")\n",
        "\n",
        "    # Check for common model directories\n",
        "    possible_model_dirs = [\n",
        "        'saved_toxicity_basic',\n",
        "        'toxicity_model_v1',\n",
        "        'automated_content_moderation_v1',\n",
        "        'influencer_brand_model'\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ Checking for model directories:\")\n",
        "    existing_dirs = []\n",
        "    for dir_name in possible_model_dirs:\n",
        "        if os.path.exists(dir_name):\n",
        "            print(f\"  âœ… {dir_name} - EXISTS\")\n",
        "            existing_dirs.append(dir_name)\n",
        "        else:\n",
        "            print(f\"  âŒ {dir_name} - NOT FOUND\")\n",
        "\n",
        "    # Check for pickle files\n",
        "    pickle_files = glob.glob(\"*.pkl\")\n",
        "    if pickle_files:\n",
        "        print(f\"\\nğŸ¥’ Pickle files found:\")\n",
        "        for file in pickle_files:\n",
        "            print(f\"  â€¢ {file}\")\n",
        "\n",
        "    return existing_dirs, pickle_files\n",
        "\n",
        "# Check what you have\n",
        "existing_models, pickle_files = check_saved_models()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsm3tFQiue3B",
        "outputId": "bfbeec92-8486-4491-b024-9fa8d0fa0d9a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” CHECKING FOR SAVED MODELS\n",
            "===================================\n",
            "ğŸ“ Files in current directory:\n",
            "  â€¢ saved_toxicity_model\n",
            "  â€¢ toxicity_model_v1\n",
            "\n",
            "ğŸ“‚ Checking for model directories:\n",
            "  âŒ saved_toxicity_basic - NOT FOUND\n",
            "  âœ… toxicity_model_v1 - EXISTS\n",
            "  âŒ automated_content_moderation_v1 - NOT FOUND\n",
            "  âŒ influencer_brand_model - NOT FOUND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST YOUR EXISTING MODELS ---\n",
        "\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "def test_existing_models():\n",
        "    \"\"\"Test the models you already have\"\"\"\n",
        "    print(\"ğŸ§ª TESTING YOUR EXISTING MODELS\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    models_to_test = [\n",
        "        (\"toxicity_model_v1\", \"directory\"),\n",
        "        (\"saved_toxicity_model\", \"file_or_directory\")\n",
        "    ]\n",
        "\n",
        "    successful_models = []\n",
        "\n",
        "    for model_path, model_type in models_to_test:\n",
        "        print(f\"\\nğŸ“¥ Testing: {model_path}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            # Try loading the model\n",
        "            model = pipeline(\"text-classification\", model=model_path)\n",
        "            print(f\"âœ… {model_path} loaded successfully!\")\n",
        "\n",
        "            # Test with sample texts\n",
        "            test_cases = [\n",
        "                (\"Safe\", \"Hello! I'd love to collaborate.\"),\n",
        "                (\"Toxic\", \"You are terrible!\"),\n",
        "                (\"Profanity\", \"fuck you\"),\n",
        "                (\"Spam\", \"Buy followers cheap!\")\n",
        "            ]\n",
        "\n",
        "            print(\"Testing results:\")\n",
        "            for category, text in test_cases:\n",
        "                result = model(text)\n",
        "                score = result[0]['score']\n",
        "                action = \"BLOCK\" if score > 0.5 else \"ALLOW\"\n",
        "                print(f\"  {category:<8} | {action:<5} | {score:.3f} | '{text}'\")\n",
        "\n",
        "            successful_models.append((model_path, model))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to load {model_path}: {e}\")\n",
        "\n",
        "    return successful_models\n",
        "\n",
        "# Test your existing models\n",
        "working_models = test_existing_models()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrd4S-xEutho",
        "outputId": "c76a618f-0f5d-40bf-ace4-278e9bc64893"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª TESTING YOUR EXISTING MODELS\n",
            "========================================\n",
            "\n",
            "ğŸ“¥ Testing: toxicity_model_v1\n",
            "------------------------------\n",
            "âœ… toxicity_model_v1 loaded successfully!\n",
            "Testing results:\n",
            "  Safe     | ALLOW | 0.001 | 'Hello! I'd love to collaborate.'\n",
            "  Toxic    | BLOCK | 0.939 | 'You are terrible!'\n",
            "  Profanity | BLOCK | 0.998 | 'fuck you'\n",
            "  Spam     | ALLOW | 0.004 | 'Buy followers cheap!'\n",
            "\n",
            "ğŸ“¥ Testing: saved_toxicity_model\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… saved_toxicity_model loaded successfully!\n",
            "Testing results:\n",
            "  Safe     | ALLOW | 0.001 | 'Hello! I'd love to collaborate.'\n",
            "  Toxic    | BLOCK | 0.939 | 'You are terrible!'\n",
            "  Profanity | BLOCK | 0.998 | 'fuck you'\n",
            "  Spam     | ALLOW | 0.004 | 'Buy followers cheap!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DETAILED TESTING OF WORKING MODELS ---\n",
        "\n",
        "def detailed_model_testing(working_models):\n",
        "    \"\"\"Run comprehensive tests on working models\"\"\"\n",
        "    print(f\"\\nğŸ”¬ COMPREHENSIVE MODEL TESTING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not working_models:\n",
        "        print(\"âŒ No working models found to test\")\n",
        "        return\n",
        "\n",
        "    # Use the first working model for detailed testing\n",
        "    model_path, model = working_models[0]\n",
        "    print(f\"ğŸ¯ Using model: {model_path}\")\n",
        "\n",
        "    # Comprehensive test cases\n",
        "    comprehensive_tests = [\n",
        "        # Safe content\n",
        "        (\"Business\", \"Hi! I'd love to collaborate on your skincare campaign.\"),\n",
        "        (\"Professional\", \"When would be a good time to discuss partnership details?\"),\n",
        "        (\"Friendly\", \"Thank you for considering me for this opportunity.\"),\n",
        "\n",
        "        # Toxic content\n",
        "        (\"Profanity\", \"fuck you and your stupid company\"),\n",
        "        (\"Insult\", \"You are a terrible person!\"),\n",
        "        (\"Harassment\", \"I hate you so much\"),\n",
        "\n",
        "        # Threats\n",
        "        (\"Threat\", \"I'll kill you\"),\n",
        "        (\"Violence\", \"I'm gonna hurt you\"),\n",
        "\n",
        "        # Spam/Scam\n",
        "        (\"Spam\", \"Buy 10k followers for just $20!\"),\n",
        "        (\"Scam\", \"Send money first via Bitcoin payment\"),\n",
        "\n",
        "        # Borderline cases\n",
        "        (\"Frustration\", \"This is really annoying\"),\n",
        "        (\"Criticism\", \"Your service sucks\"),\n",
        "        (\"Mild negative\", \"I don't like this deal\")\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ“Š DETAILED TEST RESULTS:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'CATEGORY':<12} | {'ACTION':<8} | {'SCORE':<6} | {'CONFIDENCE':<10} | TEXT\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    results_summary = {\n",
        "        \"total\": len(comprehensive_tests),\n",
        "        \"blocked\": 0,\n",
        "        \"allowed\": 0,\n",
        "        \"high_confidence\": 0\n",
        "    }\n",
        "\n",
        "    for category, text in comprehensive_tests:\n",
        "        try:\n",
        "            result = model(text)\n",
        "            score = result[0]['score']\n",
        "\n",
        "            # Determine action and confidence\n",
        "            if score > 0.7:\n",
        "                action = \"ğŸš« BLOCK\"\n",
        "                confidence = \"HIGH\"\n",
        "                results_summary[\"blocked\"] += 1\n",
        "                results_summary[\"high_confidence\"] += 1\n",
        "            elif score > 0.4:\n",
        "                action = \"âš ï¸ FLAG\"\n",
        "                confidence = \"MEDIUM\"\n",
        "                results_summary[\"blocked\"] += 1\n",
        "            else:\n",
        "                action = \"âœ… ALLOW\"\n",
        "                confidence = \"LOW\"\n",
        "                results_summary[\"allowed\"] += 1\n",
        "\n",
        "            if score > 0.8:\n",
        "                results_summary[\"high_confidence\"] += 1\n",
        "\n",
        "            print(f\"{category:<12} | {action:<8} | {score:<6.3f} | {confidence:<10} | {text[:25]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{category:<12} | ERROR    | ------ | ERROR      | {text[:25]}...\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"ğŸ“ˆ SUMMARY:\")\n",
        "    print(f\"  Total tests: {results_summary['total']}\")\n",
        "    print(f\"  Blocked/Flagged: {results_summary['blocked']}\")\n",
        "    print(f\"  Allowed: {results_summary['allowed']}\")\n",
        "    print(f\"  High confidence decisions: {results_summary['high_confidence']}\")\n",
        "\n",
        "    # Calculate accuracy estimate\n",
        "    expected_blocks = 8  # Toxic, threats, spam categories\n",
        "    actual_blocks = results_summary['blocked']\n",
        "    accuracy_estimate = min(actual_blocks / expected_blocks, 1.0) * 100\n",
        "\n",
        "    print(f\"  Estimated accuracy: {accuracy_estimate:.1f}%\")\n",
        "\n",
        "    return results_summary\n",
        "\n",
        "# Run detailed testing\n",
        "if working_models:\n",
        "    test_summary = detailed_model_testing(working_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfxp5ZYvI_c",
        "outputId": "1712d2fd-93fc-4cde-b0e8-f5039271a915"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”¬ COMPREHENSIVE MODEL TESTING\n",
            "==================================================\n",
            "ğŸ¯ Using model: toxicity_model_v1\n",
            "\n",
            "ğŸ“Š DETAILED TEST RESULTS:\n",
            "----------------------------------------------------------------------\n",
            "CATEGORY     | ACTION   | SCORE  | CONFIDENCE | TEXT\n",
            "----------------------------------------------------------------------\n",
            "Business     | âœ… ALLOW  | 0.001  | LOW        | Hi! I'd love to collabora...\n",
            "Professional | âœ… ALLOW  | 0.001  | LOW        | When would be a good time...\n",
            "Friendly     | âœ… ALLOW  | 0.001  | LOW        | Thank you for considering...\n",
            "Profanity    | ğŸš« BLOCK  | 0.998  | HIGH       | fuck you and your stupid ...\n",
            "Insult       | ğŸš« BLOCK  | 0.925  | HIGH       | You are a terrible person...\n",
            "Harassment   | ğŸš« BLOCK  | 0.930  | HIGH       | I hate you so much...\n",
            "Threat       | ğŸš« BLOCK  | 0.902  | HIGH       | I'll kill you...\n",
            "Violence     | ğŸš« BLOCK  | 0.864  | HIGH       | I'm gonna hurt you...\n",
            "Spam         | âœ… ALLOW  | 0.004  | LOW        | Buy 10k followers for jus...\n",
            "Scam         | âœ… ALLOW  | 0.004  | LOW        | Send money first via Bitc...\n",
            "Frustration  | âœ… ALLOW  | 0.041  | LOW        | This is really annoying...\n",
            "Criticism    | ğŸš« BLOCK  | 0.988  | HIGH       | Your service sucks...\n",
            "Mild negative | âœ… ALLOW  | 0.006  | LOW        | I don't like this deal...\n",
            "----------------------------------------------------------------------\n",
            "ğŸ“ˆ SUMMARY:\n",
            "  Total tests: 13\n",
            "  Blocked/Flagged: 6\n",
            "  Allowed: 7\n",
            "  High confidence decisions: 12\n",
            "  Estimated accuracy: 75.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- INTERACTIVE TESTING ---\n",
        "\n",
        "def interactive_testing(working_models):\n",
        "    \"\"\"Interactive testing interface for your models\"\"\"\n",
        "    if not working_models:\n",
        "        print(\"âŒ No working models available for interactive testing\")\n",
        "        return\n",
        "\n",
        "    model_path, model = working_models[0]\n",
        "    print(f\"ğŸ® INTERACTIVE TESTING WITH: {model_path}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Type messages to test toxicity detection.\")\n",
        "    print(\"Commands: 'quit' to exit, 'stats' for statistics\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    test_stats = {\"total\": 0, \"blocked\": 0, \"flagged\": 0, \"allowed\": 0}\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter text to test: \").strip()\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        elif user_input.lower() == 'stats':\n",
        "            print(f\"\\nğŸ“Š SESSION STATS:\")\n",
        "            print(f\"  Tests run: {test_stats['total']}\")\n",
        "            print(f\"  Blocked: {test_stats['blocked']}\")\n",
        "            print(f\"  Flagged: {test_stats['flagged']}\")\n",
        "            print(f\"  Allowed: {test_stats['allowed']}\")\n",
        "            continue\n",
        "        elif not user_input:\n",
        "            print(\"âš ï¸ Please enter some text to test\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Test the input\n",
        "            result = model(user_input)\n",
        "            score = result[0]['score']\n",
        "            test_stats[\"total\"] += 1\n",
        "\n",
        "            # Determine action\n",
        "            if score > 0.7:\n",
        "                action = \"ğŸš« BLOCKED\"\n",
        "                reason = \"High toxicity detected\"\n",
        "                test_stats[\"blocked\"] += 1\n",
        "            elif score > 0.4:\n",
        "                action = \"âš ï¸ FLAGGED\"\n",
        "                reason = \"Moderate toxicity detected\"\n",
        "                test_stats[\"flagged\"] += 1\n",
        "            else:\n",
        "                action = \"âœ… ALLOWED\"\n",
        "                reason = \"Content appears safe\"\n",
        "                test_stats[\"allowed\"] += 1\n",
        "\n",
        "            print(f\"\\nğŸ“ TEXT: '{user_input}'\")\n",
        "            print(f\"ğŸ¤– SCORE: {score:.3f} ({score*100:.1f}% toxic)\")\n",
        "            print(f\"âš¡ ACTION: {action}\")\n",
        "            print(f\"ğŸ’­ REASON: {reason}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error testing text: {e}\")\n",
        "\n",
        "# Run interactive testing\n",
        "if working_models:\n",
        "    interactive_testing(working_models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvG8gQzvP3-",
        "outputId": "bb67e58a-6954-49ff-b9e9-d0a7b89691a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ® INTERACTIVE TESTING WITH: toxicity_model_v1\n",
            "==================================================\n",
            "Type messages to test toxicity detection.\n",
            "Commands: 'quit' to exit, 'stats' for statistics\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter text to test: i have to know about you\n",
            "\n",
            "ğŸ“ TEXT: 'i have to know about you'\n",
            "ğŸ¤– SCORE: 0.001 (0.1% toxic)\n",
            "âš¡ ACTION: âœ… ALLOWED\n",
            "ğŸ’­ REASON: Content appears safe\n",
            "\n",
            "Enter text to test: can you provide your website link\n",
            "\n",
            "ğŸ“ TEXT: 'can you provide your website link'\n",
            "ğŸ¤– SCORE: 0.001 (0.1% toxic)\n",
            "âš¡ ACTION: âœ… ALLOWED\n",
            "ğŸ’­ REASON: Content appears safe\n",
            "\n",
            "Enter text to test: i will kill you\n",
            "\n",
            "ğŸ“ TEXT: 'i will kill you'\n",
            "ğŸ¤– SCORE: 0.907 (90.7% toxic)\n",
            "âš¡ ACTION: ğŸš« BLOCKED\n",
            "ğŸ’­ REASON: High toxicity detected\n",
            "\n",
            "Enter text to test: you are tribble\n",
            "\n",
            "ğŸ“ TEXT: 'you are tribble'\n",
            "ğŸ¤– SCORE: 0.681 (68.1% toxic)\n",
            "âš¡ ACTION: âš ï¸ FLAGGED\n",
            "ğŸ’­ REASON: Moderate toxicity detected\n",
            "\n",
            "Enter text to test: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zaXUn4b6vVb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}